{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "758cdb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.60456741 0.0940027  1.72436855 1.71085905 1.54082385 0.16649924\n",
      " 0.1588792 ]\n",
      "Sum: 6.999999999999999\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import random_correlation\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "def generate_random_numbers(n):\n",
    "    if n <= 0:\n",
    "        raise ValueError(\"n must be a positive integer\")\n",
    "    numbers = np.random.random(n)    \n",
    "    numbers = numbers / np.sum(numbers) * (n)\n",
    "    return numbers\n",
    "\n",
    "n = 7\n",
    "random_numbers = generate_random_numbers(n)\n",
    "print(random_numbers)\n",
    "print(\"Sum:\", np.sum(random_numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "86029e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<scipy.stats._multivariate.multivariate_normal_frozen at 0x7f5c6d47c880>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = np.random.default_rng()\n",
    "covmat_Z = random_correlation.rvs(random_numbers, random_state=rng)\n",
    "print(covmat_Z.shape)\n",
    "mvnorm = stats.multivariate_normal(mean=np.zeros(covmat_Z.shape[0]), cov=covmat_Z) \n",
    "mvnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8a9672cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def gen_mean_std():\n",
    "    return np.round(np.random.uniform(-1, 1), 2) , np.round(np.random.uniform(-1, 1), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ddf5d446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         X1        X2        X3        X4  A  B  Y\n",
      "0 -1.579340 -0.784797  0.606719  1.151511  0  0  1\n",
      "1 -1.328865 -0.741596  0.463265  0.612546  0  0  0\n",
      "2 -1.357357 -0.636942  0.228926  0.166021  0  1  0\n",
      "3 -0.936441 -0.735279  0.559075  0.443410  0  0  0\n",
      "4 -1.994651 -0.637910 -0.179197  1.050773  0  0  1\n"
     ]
    }
   ],
   "source": [
    "m1 = stats.norm(*gen_mean_std())\n",
    "m2 = stats.norm(*gen_mean_std())\n",
    "m3 = stats.norm(*gen_mean_std())\n",
    "m4 = stats.norm(*gen_mean_std())\n",
    "m5 = stats.uniform()\n",
    "m6 = stats.uniform()\n",
    "m7 = stats.uniform()\n",
    "\n",
    "p_a = np.round(np.random.uniform(0.1, 0.9), 1)\n",
    "tau_a = 1 - p_a\n",
    "\n",
    "p_b = np.round(np.random.uniform(0.1, 0.9), 1)\n",
    "tau_b = 1-p_b\n",
    "\n",
    "p_y = np.round(np.random.uniform(0.1, 0.9), 1)\n",
    "tau_y = 1 - p_y \n",
    "\n",
    "for i in range(1):\n",
    "    z = mvnorm.rvs(100000)\n",
    "    u = stats.norm.cdf(z)\n",
    "    x1 = m1.ppf(u[:, 0])\n",
    "    x2 = m2.ppf(u[:, 1])\n",
    "    x3 = m3.ppf(u[:, 2])\n",
    "    x4 = m4.ppf(u[:, 3])\n",
    "    x5 = m5.ppf(u[:, 4])\n",
    "    x6 = m6.ppf(u[:, 5])\n",
    "    x7 = m7.ppf(u[:, 6])\n",
    "    a = np.zeros(len(x5), dtype=int)\n",
    "    a[x5 >= tau_a] = 1\n",
    "    b = np.zeros(len(x6), dtype=int)\n",
    "    b[x6 >= tau_b] = 1\n",
    "    y = np.zeros(len(x7), dtype=int)\n",
    "    y[x7 >= tau_y] = 1\n",
    "    data = pd.DataFrame({'X1':x1, 'X2':x2, 'X3':x3, 'X4':x4, 'A':a, 'B':b, 'Y':y})\n",
    "#     data.to_csv('synthetic_data_v2010_' + str(i) + '.csv', index=False) \n",
    "    print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a4d3096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import random_correlation\n",
    "import scipy.stats as stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_n_random_numbers_with_sum_n(n):\n",
    "    if n <= 0:\n",
    "        raise ValueError(\"n must be a positive integer\")\n",
    "    numbers = np.random.random(n)    \n",
    "    numbers = numbers / np.sum(numbers) * (n)\n",
    "    return numbers\n",
    "\n",
    "def gen_mean_std():\n",
    "    return np.round(np.random.uniform(-1, 1), 2) , np.round(np.random.uniform(0, 1), 2)\n",
    "\n",
    "def generate_simulated_data(n=10000, n_datasets=100, num_non_sensitive=4, num_sensitive=2, latent_dim=None, non_sensitive_dist='normal'):\n",
    "    \"\"\"\n",
    "    Generate a list of simulated datasets with both non-sensitive and sensitive variables.\n",
    "\n",
    "    Parameters:\n",
    "    - n (int): The number of samples in each dataset. Default is 10,000.\n",
    "    - n_datasets (int): The number of datasets to sample. Default is 100.\n",
    "    - num_non_sensitive (int): The number of non-sensitive variables in each dataset. Default is 4.\n",
    "    - num_sensitive (int): The number of sensitive variables in each dataset. Default is 2.\n",
    "    - non_sensitive_dist (str): Distribution type for non-sensitive variables. Options are:\n",
    "      - 'normal': Normal distribution\n",
    "      - 'exp': Exponential distribution\n",
    "      - 'gamma': Gamma distribution\n",
    "      - 'beta': Beta distribution\n",
    "      - 'lognorm': Log-normal distribution\n",
    "      Default is 'normal'.\n",
    "\n",
    "    Returns:\n",
    "    - List[pd.DataFrame]: A list containing `n_datasets` DataFrames, each representing a simulated dataset with the following columns:\n",
    "      - Non-sensitive variables: `X1`, `X2`, ..., up to `X{num_non_sensitive}`\n",
    "      - Sensitive variables: `S1`, `S2`, ..., up to `S{num_sensitive}`\n",
    "      - Binary outcome variable: `Y`\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If an unrecognized distribution is provided for `non_sensitive_dist`.\n",
    "\n",
    "    Notes:\n",
    "    - The function generates a multivariate normal distribution for the latent variables and transforms them to produce the non-sensitive and sensitive variables.\n",
    "    - Thresholds for sensitive variables and the binary outcome variable are determined based on uniform distributions and applied to the generated data.\n",
    "\n",
    "    Example:\n",
    "    >>> datasets = generate_simulated_data(n=5000, n_datasets=10, num_non_sensitive=3, num_sensitive=2, non_sensitive_dist='gamma')\n",
    "    >>> len(datasets)\n",
    "    10\n",
    "    >>> datasets[0].head()\n",
    "         X1        X2        X3  S1  S2  Y\n",
    "    0  ...       ...       ...   0   1  0\n",
    "    1  ...       ...       ...   1   0  1\n",
    "    \"\"\"\n",
    "    total_vars = num_non_sensitive + num_sensitive + 1\n",
    "    random_numbers = generate_n_random_numbers_with_sum_n(total_vars)\n",
    "    rng = np.random.default_rng()\n",
    "    covmat_Z = random_correlation.rvs(random_numbers, random_state=rng)\n",
    "    mvnorm = stats.multivariate_normal(mean=np.zeros(covmat_Z.shape[0]), cov=covmat_Z) \n",
    "\n",
    "    non_sensitive_dists = []\n",
    "    for i in range(num_non_sensitive):\n",
    "        if non_sensitive_dist == 'normal':\n",
    "            non_sensitive_dists.append(stats.norm(*gen_mean_std()))\n",
    "        elif non_sensitive_dist == 'exp':\n",
    "            scale = 1  \n",
    "            non_sensitive_dists.append(stats.expon(scale=scale))\n",
    "        elif non_sensitive_dist == 'gamma':\n",
    "            shape = 2.0\n",
    "            scale = 1.0\n",
    "            non_sensitive_dists.append(stats.gamma(shape, scale=scale))\n",
    "        elif non_sensitive_dist == 'beta':\n",
    "            a = 2.0\n",
    "            b = 5.0\n",
    "            non_sensitive_dists.append(stats.beta(a,b))\n",
    "        elif non_sensitive_dist == 'lognorm':\n",
    "            shape = 0.954\n",
    "            scale = 1.0\n",
    "            non_sensitive_dists.append(stats.lognorm(s=shape, scale=scale))\n",
    "        else:\n",
    "            raise ValueError(f\"Unrecognized distribution: {non_sensitive_dist}\")\n",
    "\n",
    "    sensitive_dists = []\n",
    "    for i in range(num_sensitive):\n",
    "        sensitive_dists.append(stats.uniform())\n",
    "\n",
    "    distY = stats.uniform()\n",
    "    \n",
    "    tau_sensitive = []\n",
    "    for i in range(num_sensitive):\n",
    "        p = np.round(np.random.uniform(0.1, 0.9), 1)\n",
    "        tau_sensitive.append(1 - p)\n",
    "\n",
    "    p_y = np.round(np.random.uniform(0.1, 0.9), 1)\n",
    "    tau_y = 1 - p_y \n",
    "    \n",
    "    datasets = []\n",
    "    \n",
    "    for i in tqdm(range(n_datasets)):\n",
    "        z = mvnorm.rvs(n)\n",
    "        u = stats.norm.cdf(z)\n",
    "        \n",
    "        X = []\n",
    "        for i in range(num_non_sensitive):\n",
    "            X.append(non_sensitive_dists[i].ppf(u[:, i]))\n",
    "            \n",
    "        S = []\n",
    "        for i in range(num_sensitive):\n",
    "            sensitive_data = sensitive_dists[i].ppf(u[:, num_non_sensitive + i])\n",
    "            sensitive_data_bin = np.zeros(len(sensitive_data), dtype=int)\n",
    "            sensitive_data_bin[sensitive_data >= tau_sensitive[i]] = 1\n",
    "            S.append(sensitive_data_bin)\n",
    "        xY = distY.ppf(u[:, num_non_sensitive + num_sensitive])\n",
    "        y = np.zeros(len(xY), dtype=int)\n",
    "        y[xY >= tau_y] = 1     \n",
    "        \n",
    "        datasets.append(pd.DataFrame({f'X{i+1}': X[i] for i in range(len(X))} | {f'S{i+1}': S[i] for i in range(len(S))} | {'Y':y}))\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b898aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 71.51it/s]\n"
     ]
    }
   ],
   "source": [
    "datasets = generate_simulated_data(n=10000, n_datasets=10, num_non_sensitive=4, num_sensitive=1, latent_dim=None, non_sensitive_dist='normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdfa03c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5057"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[6].Y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53af86ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4993\n",
      "0.4927\n",
      "0.4995\n",
      "0.5061\n",
      "0.504\n",
      "0.5022\n",
      "0.5057\n",
      "0.5025\n",
      "0.5027\n",
      "0.4959\n"
     ]
    }
   ],
   "source": [
    "for i in datasets:\n",
    "    print(i.Y.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47a6b7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5959\n",
      "0.5992\n",
      "0.6011\n",
      "0.5978\n",
      "0.5919\n",
      "0.6017\n",
      "0.5955\n",
      "0.6024\n",
      "0.5998\n",
      "0.6\n"
     ]
    }
   ],
   "source": [
    "for i in datasets:\n",
    "    print(i.S1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314b06f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad8d46a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eca4b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfad701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aace33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffa5701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b2ee1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc6b6589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "121837a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "885dcf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, input_dim-1)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "#         return self.relu(self.fc1(x))\n",
    "        return self.fc1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32f4994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Predictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.fc1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6b5c55ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adversary(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Adversary, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.fc1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b62a7216",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.X = torch.tensor(dataframe[['X1', 'X2', 'X3', 'X4', 'S1']].values, dtype=torch.float32)\n",
    "#         self.A = torch.tensor(dataframe[['A']].values, dtype=torch.float32)\n",
    "#         self.B = torch.tensor(dataframe[['B']].values, dtype=torch.float32)\n",
    "        self.Y = torch.tensor(dataframe[['Y']].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "#         return self.X[idx], self.A[idx], self.B[idx], self.Y[idx]\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "dataset = CustomDataset(datasets[0])\n",
    "dataloader = DataLoader(dataset, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cc0394bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(5)\n",
    "predictor = Predictor(4)\n",
    "adversary = Adversary(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fb4a44a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "criterion_enc = nn.MSELoss()\n",
    "criterion_pred = nn.BCELoss()\n",
    "# criterion_adv = nn.BCELoss()\n",
    "criterion_adv = nn.L1Loss()\n",
    "\n",
    "optimizer_pred = optim.Adam(predictor.parameters(), lr=learning_rate)\n",
    "optimizer_enc = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "# optimizer = optim.Adam(list(predictor.parameters()), lr=learning_rate)\n",
    "# optimizer = optim.Adam(list(encoder.parameters()) + list(predictor.parameters()), lr=learning_rate)\n",
    "optimizer_adv = optim.Adam(list(adversary.parameters()), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b490fc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 5])\n",
      "torch.Size([256, 1])\n"
     ]
    }
   ],
   "source": [
    "for a,b in dataloader:\n",
    "    print(a.shape)\n",
    "    print(b.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f5884332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adversary(\n",
       "  (fc1): Linear(in_features=4, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "encoder.train()\n",
    "predictor.train()\n",
    "adversary.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e730f0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len_A0Y0: 46\n",
      "len_A0Y1: 55\n",
      "len_A1Y0: 75\n",
      "len_A1Y1: 80\n",
      "Sum of all lengths: 256\n",
      "AY_proportion: [[0.1796875, 0.21484375], [0.29296875, 0.3125]]\n",
      "A_prop: [0.39453125, 0.60546875]\n",
      "Y_prop: [0.47265625, 0.52734375]\n",
      "wts: torch.Size([256, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: Error detected in AddmmBackward0. Traceback of forward call that caused the error:\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/ipykernel/__main__.py\", line 5, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_29407/2726946812.py\", line 36, in <module>\n",
      "    adv_pred = adversary(x_recon)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_29407/2240300564.py\", line 7, in forward\n",
      "    return self.sigmoid(self.fc1(x))\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      " (Triggered internally at /opt/conda/conda-bld/pytorch_1696595231861/work/torch/csrc/autograd/python_anomaly_mode.cpp:114.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [4, 1]], which is output 0 of AsStridedBackward0, is at version 4002; expected version 4001 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m optimizer_adv\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m aud_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [4, 1]], which is output 0 of AsStridedBackward0, is at version 4002; expected version 4001 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for data, labels in dataloader:\n",
    "        A = data[:,-1].view(-1,1).float()\n",
    "        len_A0Y0 = ((A == 0) & (labels == 0)).sum().item()\n",
    "        len_A0Y1 = ((A == 0) & (labels == 1)).sum().item()\n",
    "        len_A1Y0 = ((A == 1) & (labels == 0)).sum().item()\n",
    "        len_A1Y1 = ((A == 1) & (labels == 1)).sum().item()\n",
    "        \n",
    "        total_length = len_A0Y0 + len_A0Y1 + len_A1Y0 + len_A1Y1\n",
    "\n",
    "        A0Y0 = len_A0Y0 / total_length\n",
    "        A0Y1 = len_A0Y1 / total_length\n",
    "        A1Y0 = len_A1Y0 / total_length\n",
    "        A1Y1 = len_A1Y1 / total_length\n",
    "        \n",
    "        AY_proportion = [[A0Y0, A0Y1], [A1Y0, A1Y1]]\n",
    "        A_prop = [AY_proportion[0][0] + AY_proportion[0][1], AY_proportion[1][0] + AY_proportion[1][1]]\n",
    "        Y_prop = [AY_proportion[0][0] + AY_proportion[1][0], AY_proportion[0][1] + AY_proportion[1][1]]\n",
    "\n",
    "        wts = A_prop[0] * (1 - A) + A_prop[1] * A\n",
    "        print(f\"len_A0Y0: {len_A0Y0}\")\n",
    "        print(f\"len_A0Y1: {len_A0Y1}\")\n",
    "        print(f\"len_A1Y0: {len_A1Y0}\")\n",
    "        print(f\"len_A1Y1: {len_A1Y1}\")\n",
    "        print(f\"Sum of all lengths: {total_length}\")\n",
    "        print(f\"AY_proportion: {AY_proportion}\")\n",
    "        print(f\"A_prop: {A_prop}\")\n",
    "        print(f\"Y_prop: {Y_prop}\")\n",
    "        print(f\"wts: {wts.shape}\")\n",
    "        \n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        \n",
    "        x_recon = encoder(data)\n",
    "        y_pred = predictor(x_recon)\n",
    "        adv_pred = adversary(x_recon)\n",
    "        \n",
    "        recon_loss = criterion_enc(x_recon, data[:,:4])\n",
    "        \n",
    "#         class_loss = class_coeff * criterion_pred(y_pred, labels)\n",
    "        class_loss = 5 * criterion_pred(y_pred, labels)\n",
    "#         aud_loss = -fair_coeff * criterion_adv(A, A_logits)\n",
    "        aud_loss = -5 * criterion_adv(adv_pred, A)\n",
    "    \n",
    "        weighted_aud_loss = torch.mean(aud_loss * torch.squeeze(wts))\n",
    "\n",
    "        loss = recon_loss + class_loss + weighted_aud_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        torch.nn.utils.clip_grad_norm_(list(encoder.parameters())+list(predictor.parameters()), 5.0)\n",
    "        optimizer.step()\n",
    "        aud_steps = 10\n",
    "        for i in range(aud_steps):\n",
    "            optimizer_adv.zero_grad()\n",
    "            if i != aud_steps - 1:\n",
    "                loss.backward(retain_graph=True)\n",
    "            else:\n",
    "                loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(adversary.parameters(), 5.0)\n",
    "            optimizer_adv.step()\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3600543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef370c57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40aad70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d5b18e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs=100\n",
    "# for epoch in range(num_epochs):\n",
    "    \n",
    "#     for data, labels in dataloader: \n",
    "#         adversary.zero_grad()\n",
    "#         x_recon = encoder(data)\n",
    "#         adv_pred = adversary(x_recon)\n",
    "#         la = criterion_adv(adv_pred, data[:,4].view(-1,1).float())\n",
    "#         la.backward()\n",
    "        \n",
    "#         optimizer_adv.step()\n",
    "        \n",
    "#     for data, labels in dataloader: \n",
    "#         pass\n",
    "    \n",
    "#     encoder.zero_grad()\n",
    "#     predictor.zero_grad()\n",
    "#     x_recon = encoder(data)\n",
    "#     y_pred = predictor(x_recon)\n",
    "#     adv_pred = adversary(x_recon)\n",
    "\n",
    "#     lx = criterion_enc(x_recon, data[:,:4])\n",
    "#     lp = criterion_pred(y_pred, labels)\n",
    "#     la = criterion_adv(adv_pred, data[:,4].view(-1,1).float())\n",
    "    \n",
    "#     combined_loss = lx + lp - la\n",
    "#     combined_loss.backward()\n",
    "\n",
    "#     optimizer_pred.step()\n",
    "#     optimizer_enc.step()\n",
    "\n",
    "#     print(f'Epoch [{epoch+1}/{num_epochs}], Loss Adv: {la.item():.4f}, Loss Recon: {lx.item():.4f}, Loss P: {lp.item():.4f}, Loss Comb: {combined_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4382633b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 4])\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "df_original = datasets[index].copy()\n",
    "\n",
    "dataset = CustomDataset(df_original)\n",
    "dataloader = DataLoader(dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "encoder_outputs = []\n",
    "encoder.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_X, _ in dataloader:\n",
    "        output = encoder(batch_X)\n",
    "        encoder_outputs.append(output)\n",
    "encoder_outputs = torch.cat(encoder_outputs, dim=0)\n",
    "print(encoder_outputs.shape)\n",
    "\n",
    "encoder_outputs_np = encoder_outputs.numpy()\n",
    "df = pd.DataFrame(encoder_outputs_np, columns=['X1', 'X2', 'X3', 'X4'])\n",
    "\n",
    "df_A_B = df_original[['S1', 'Y']].reset_index(drop=True)\n",
    "df = pd.concat([df, df_A_B], axis=1)\n",
    "\n",
    "correlation_matrix = df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "aa97bc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20, 8))\n",
    "# plt.subplot(1,2,1)\n",
    "# sns.heatmap(datasets[0].corr(), annot=True, cmap='coolwarm', fmt='.4f', vmin=-1, vmax=1)\n",
    "# plt.subplot(1,2,2)\n",
    "# sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.4f', vmin=-1, vmax=1)\n",
    "# plt.title('Pearson Correlation Heatmap')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80abf307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0defb26c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
